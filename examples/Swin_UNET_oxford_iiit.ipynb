{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oxford IIIT image segmentation with SwinUNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from keras_vision_transformer import swin_layers\n",
    "from keras_vision_transformer import transformer_layers\n",
    "from keras_vision_transformer import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example applies the dataset of [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/) (Parkhi et al. 2012). This dataset contains images of pets and their pixel-wise mask that indicates (1) pixels belonging to the pet, (2) pixels bordering the pet, and (3) surrounding pixels.\n",
    "\n",
    "A semantic segmentation problem is proposed; it takes images as inputs and predicts the classification probability of the three pixel-wise masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the indicator of a fresh run\n",
    "first_time_running = False\n",
    "\n",
    "# user-specified working directory\n",
    "filepath = '/system/drive/oxford_iiit/'\n",
    "\n",
    "filepath = r'C:\\dev\\repos\\ImageDecomposition\\keras-vision-transformer\\examples\\datasets\\SEMs\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if first_time_running:\n",
    "    # downloading and executing data files\n",
    "    import tarfile\n",
    "    import urllib.request\n",
    "    \n",
    "    filename_image = filepath+'images.tar.gz'\n",
    "    filename_target = filepath+'annotations.tar.gz'\n",
    "    \n",
    "    urllib.request.urlretrieve('http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz', filename_image);\n",
    "    urllib.request.urlretrieve('https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz', filename_target);\n",
    "    \n",
    "    with tarfile.open(filename_image, \"r:gz\") as tar_io:\n",
    "        tar_io.extractall(path=filepath)\n",
    "    with tarfile.open(filename_target, \"r:gz\") as tar_io:\n",
    "        tar_io.extractall(path=filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Swin-UNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two functions are provided for customizing the Swin-UNET:\n",
    "    \n",
    "* `swin_transformer_stack`: a function that stacks multiple Swin Transformers.\n",
    "* `swin_unet_2d_base`: the base architecture of the Swin-UNET with down-/upsampling levels and skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swin_transformer_stack(X, stack_num, embed_dim, num_patch, num_heads, window_size, num_mlp, shift_window=True, name=''):\n",
    "    '''\n",
    "    Stacked Swin Transformers that share the same token size.\n",
    "    \n",
    "    Alternated Window-MSA and Swin-MSA will be configured if `shift_window=True`, Window-MSA only otherwise.\n",
    "    *Dropout is turned off.\n",
    "    '''\n",
    "    # Turn-off dropouts\n",
    "    mlp_drop_rate = 0 # Droupout after each MLP layer\n",
    "    attn_drop_rate = 0 # Dropout after Swin-Attention\n",
    "    proj_drop_rate = 0 # Dropout at the end of each Swin-Attention block, i.e., after linear projections\n",
    "    drop_path_rate = 0 # Drop-path within skip-connections\n",
    "    \n",
    "    qkv_bias = True # Convert embedded patches to query, key, and values with a learnable additive value\n",
    "    qk_scale = None # None: Re-scale query based on embed dimensions per attention head # Float for user specified scaling factor\n",
    "    \n",
    "    if shift_window:\n",
    "        shift_size = window_size // 2\n",
    "    else:\n",
    "        shift_size = 0\n",
    "    \n",
    "    for i in range(stack_num):\n",
    "    \n",
    "        if i % 2 == 0:\n",
    "            shift_size_temp = 0\n",
    "        else:\n",
    "            shift_size_temp = shift_size\n",
    "\n",
    "        X = swin_layers.SwinTransformerBlock(dim=embed_dim, \n",
    "                                             num_patch=num_patch, \n",
    "                                             num_heads=num_heads, \n",
    "                                             window_size=window_size, \n",
    "                                             shift_size=shift_size_temp, \n",
    "                                             num_mlp=num_mlp, \n",
    "                                             qkv_bias=qkv_bias, \n",
    "                                             qk_scale=qk_scale,\n",
    "                                             mlp_drop=mlp_drop_rate, \n",
    "                                             attn_drop=attn_drop_rate, \n",
    "                                             proj_drop=proj_drop_rate, \n",
    "                                             drop_path_prob=drop_path_rate, \n",
    "                                             name='name{}'.format(i))(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def swin_unet_2d_base(input_tensor, filter_num_begin, depth, stack_num_down, stack_num_up, \n",
    "                      patch_size, num_heads, window_size, num_mlp, shift_window=True, name='swin_unet'):\n",
    "    '''\n",
    "    The base of Swin-UNET.\n",
    "    \n",
    "    The general structure:\n",
    "    \n",
    "    1. Input image --> a sequence of patches --> tokenize these patches\n",
    "    2. Downsampling: swin-transformer --> patch merging (pooling)\n",
    "    3. Upsampling: concatenate --> swin-transfprmer --> patch expanding (unpooling)\n",
    "    4. Model head\n",
    "    \n",
    "    '''\n",
    "    # Compute number be patches to be embeded\n",
    "    input_size = input_tensor.shape.as_list()[1:]\n",
    "    num_patch_x = input_size[0]//patch_size[0]\n",
    "    num_patch_y = input_size[1]//patch_size[1]\n",
    "    \n",
    "    # Number of Embedded dimensions\n",
    "    embed_dim = filter_num_begin\n",
    "    \n",
    "    depth_ = depth\n",
    "    \n",
    "    X_skip = []\n",
    "\n",
    "    X = input_tensor\n",
    "    \n",
    "    # Patch extraction\n",
    "    X = transformer_layers.patch_extract(patch_size)(X)\n",
    "\n",
    "    # Embed patches to tokens\n",
    "    X = transformer_layers.patch_embedding(num_patch_x*num_patch_y, embed_dim)(X)\n",
    "    \n",
    "    # The first Swin Transformer stack\n",
    "    X = swin_transformer_stack(X, \n",
    "                               stack_num=stack_num_down, \n",
    "                               embed_dim=embed_dim, \n",
    "                               num_patch=(num_patch_x, num_patch_y), \n",
    "                               num_heads=num_heads[0], \n",
    "                               window_size=window_size[0], \n",
    "                               num_mlp=num_mlp, \n",
    "                               shift_window=shift_window, \n",
    "                               name='{}_swin_down0'.format(name))\n",
    "    X_skip.append(X)\n",
    "    \n",
    "    # Downsampling blocks\n",
    "    for i in range(depth_-1):\n",
    "        \n",
    "        # Patch merging\n",
    "        X = transformer_layers.patch_merging((num_patch_x, num_patch_y), embed_dim=embed_dim, name='down{}'.format(i))(X)\n",
    "        \n",
    "        # update token shape info\n",
    "        embed_dim = embed_dim*2\n",
    "        num_patch_x = num_patch_x//2\n",
    "        num_patch_y = num_patch_y//2\n",
    "        \n",
    "        # Swin Transformer stacks\n",
    "        X = swin_transformer_stack(X, \n",
    "                                   stack_num=stack_num_down, \n",
    "                                   embed_dim=embed_dim, \n",
    "                                   num_patch=(num_patch_x, num_patch_y), \n",
    "                                   num_heads=num_heads[i+1], \n",
    "                                   window_size=window_size[i+1], \n",
    "                                   num_mlp=num_mlp, \n",
    "                                   shift_window=shift_window, \n",
    "                                   name='{}_swin_down{}'.format(name, i+1))\n",
    "        \n",
    "        # Store tensors for concat\n",
    "        X_skip.append(X)\n",
    "        \n",
    "    # reverse indexing encoded tensors and hyperparams\n",
    "    X_skip = X_skip[::-1]\n",
    "    num_heads = num_heads[::-1]\n",
    "    window_size = window_size[::-1]\n",
    "    \n",
    "    # upsampling begins at the deepest available tensor\n",
    "    X = X_skip[0]\n",
    "    \n",
    "    # other tensors are preserved for concatenation\n",
    "    X_decode = X_skip[1:]\n",
    "    \n",
    "    depth_decode = len(X_decode)\n",
    "    \n",
    "    for i in range(depth_decode):\n",
    "        \n",
    "        # Patch expanding\n",
    "        X = transformer_layers.patch_expanding(num_patch=(num_patch_x, num_patch_y), \n",
    "                                               embed_dim=embed_dim, \n",
    "                                               upsample_rate=2, \n",
    "                                               return_vector=True)(X)\n",
    "        \n",
    "\n",
    "        # update token shape info\n",
    "        embed_dim = embed_dim//2\n",
    "        num_patch_x = num_patch_x*2\n",
    "        num_patch_y = num_patch_y*2\n",
    "        \n",
    "        # Concatenation and linear projection\n",
    "        X = concatenate([X, X_decode[i]], axis=-1, name='{}_concat_{}'.format(name, i))\n",
    "        X = Dense(embed_dim, use_bias=False, name='{}_concat_linear_proj_{}'.format(name, i))(X)\n",
    "        \n",
    "        # Swin Transformer stacks\n",
    "        X = swin_transformer_stack(X, \n",
    "                                   stack_num=stack_num_up, \n",
    "                                   embed_dim=embed_dim, \n",
    "                                   num_patch=(num_patch_x, num_patch_y), \n",
    "                                   num_heads=num_heads[i], \n",
    "                                   window_size=window_size[i], \n",
    "                                   num_mlp=num_mlp, \n",
    "                                   shift_window=shift_window, \n",
    "                                   name='{}_swin_up{}'.format(name, i))\n",
    "        \n",
    "    # The last expanding layer; it produces full-size feature maps based on the patch size\n",
    "    # !!! <--- \"patch_size[0]\" is used; it assumes patch_size = (size, size)\n",
    "    \n",
    "    X = transformer_layers.patch_expanding(num_patch=(num_patch_x, num_patch_y), \n",
    "                                           embed_dim=embed_dim, \n",
    "                                           upsample_rate=patch_size[0], \n",
    "                                           return_vector=False)(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Hyperparameters of the Swin-UNET are listed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_num_begin = 128     # number of channels in the first downsampling block; it is also the number of embedded dimensions\n",
    "depth = 4                  # the depth of SwinUNET; depth=4 means three down/upsampling levels and a bottom level \n",
    "stack_num_down = 2         # number of Swin Transformers per downsampling level\n",
    "stack_num_up = 2           # number of Swin Transformers per upsampling level\n",
    "patch_size = (4, 4)        # Extract 4-by-4 patches from the input image. Height and width of the patch must be equal.\n",
    "num_heads = [4, 8, 8, 8]   # number of attention heads per down/upsampling level\n",
    "window_size = [4, 2, 2, 2] # the size of attention window per down/upsampling level\n",
    "num_mlp = 512              # number of MLP nodes within the Transformer\n",
    "shift_window=True          # Apply window shifting, i.e., Swin-MSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input section\n",
    "input_size = (128, 128, 3)\n",
    "IN = Input(input_size)\n",
    "\n",
    "# Base architecture\n",
    "X = swin_unet_2d_base(IN, filter_num_begin, depth, stack_num_down, stack_num_up, \n",
    "                      patch_size, num_heads, window_size, num_mlp, \n",
    "                      shift_window=shift_window, name='swin_unet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output section\n",
    "n_labels = 3\n",
    "OUT = Conv2D(n_labels, kernel_size=1, use_bias=False, activation='softmax')(X)\n",
    "\n",
    "# Model configuration\n",
    "model = Model(inputs=[IN,], outputs=[OUT,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "# <---- !!! gradient clipping is important\n",
    "opt = keras.optimizers.Adam(learning_rate=1e-4, clipvalue=0.5)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n",
    "The input of RGB images are resized to 128-by-128 through the nearest neighbour scheme, and then normalized to the interval of [0, 1]. The training target of pixel-wise masks are resized similarly.\n",
    "\n",
    "A random split is applied with 80%, 10%, 10% of the samples are assigned for training, validation, and testing, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data_process(input_array):\n",
    "    '''converting pixel vales to [0, 1]'''\n",
    "    return input_array/255.\n",
    "\n",
    "def target_data_process(target_array):\n",
    "    '''Converting tri-mask of {1, 2, 3} to three categories.'''\n",
    "    return keras.utils.to_categorical(target_array-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_names = np.array(sorted(glob(filepath+'images/*.jpg')))\n",
    "label_names = np.array(sorted(glob(filepath+'annotations/trimaps/*.png')))\n",
    "\n",
    "L = len(sample_names)\n",
    "ind_all = utils.shuffle_ind(L)\n",
    "\n",
    "L_train = int(0.8*L); L_valid = int(0.1*L); L_test = L - L_train - L_valid\n",
    "ind_train = ind_all[:L_train]; ind_valid = ind_all[L_train:L_train+L_valid]; ind_test = ind_all[L_train+L_valid:]\n",
    "\n",
    "ind_train, ind_valid, ind_test= ind_all,ind_all,ind_all\n",
    "\n",
    "print(\"Training:validation:testing = {}:{}:{}\".format(L_train, L_valid, L_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_input = input_data_process(utils.image_to_array(sample_names[ind_valid], size=128, channel=3))\n",
    "valid_target = target_data_process(utils.image_to_array(label_names[ind_valid], size=128, channel=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = input_data_process(utils.image_to_array(sample_names[ind_test], size=128, channel=3))\n",
    "test_target = target_data_process(utils.image_to_array(label_names[ind_test], size=128, channel=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The segmentation model is trained with fixed 15 epoches. Each epoch containts 100 batches and each batch contains 32 samples.\n",
    "\n",
    "*The training process here is far from systematic, and is provided for illustration purposes only.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_epoch = 15 # number of epoches\n",
    "N_batch = 100 # number of batches per epoch\n",
    "N_sample = 32 # number of samples per batch\n",
    "\n",
    "tol = 0 # current early stopping patience\n",
    "max_tol = 3 # the max-allowed early stopping patience\n",
    "min_del = 0 # the lowest acceptable loss value reduction \n",
    "\n",
    "# loop over epoches\n",
    "for epoch in range(N_epoch):\n",
    "    \n",
    "    # initial loss record\n",
    "    if epoch == 0:\n",
    "        y_pred = model.predict([valid_input])\n",
    "        record = np.mean(keras.losses.categorical_crossentropy(valid_target, y_pred))\n",
    "        print('\\tInitial loss = {}'.format(record))\n",
    "    \n",
    "    # loop over batches\n",
    "    for step in range(N_batch):\n",
    "        # selecting smaples for the current batch\n",
    "        ind_train_shuffle = utils.shuffle_ind(L_train)[:N_sample]\n",
    "        \n",
    "        # batch data formation\n",
    "        ## augmentation is not applied\n",
    "        train_input = input_data_process(\n",
    "            utils.image_to_array(sample_names[ind_train][ind_train_shuffle], size=128, channel=3))\n",
    "        train_target = target_data_process(\n",
    "            utils.image_to_array(label_names[ind_train][ind_train_shuffle], size=128, channel=1))\n",
    "        \n",
    "        # train on batch\n",
    "        loss_ = model.train_on_batch([train_input,], [train_target,])\n",
    "#         if np.isnan(loss_):\n",
    "#             print(\"Training blow-up\")\n",
    "\n",
    "        # ** training loss is not stored ** #\n",
    "        \n",
    "    # epoch-end validation\n",
    "    y_pred = model.predict([valid_input])\n",
    "    record_temp = np.mean(keras.losses.categorical_crossentropy(valid_target, y_pred))\n",
    "    # ** validation loss is not stored ** #\n",
    "    \n",
    "    # if loss is reduced\n",
    "    if record - record_temp > min_del:\n",
    "        print('Validation performance is improved from {} to {}'.format(record, record_temp))\n",
    "        record = record_temp; # update the loss record\n",
    "        tol = 0; # refresh early stopping patience\n",
    "        # ** model checkpoint is not stored ** #\n",
    "\n",
    "    # if loss not reduced\n",
    "    else:\n",
    "        print('Validation performance {} is NOT improved'.format(record_temp))\n",
    "        tol += 1\n",
    "        if tol >= max_tol:\n",
    "            print('Early stopping')\n",
    "            break;\n",
    "        else:\n",
    "            # Pass to the next epoch\n",
    "            continue;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "The testing set performance is evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([test_input,])\n",
    "print('Testing set cross-entropy loss = {}'.format(np.mean(keras.losses.categorical_crossentropy(test_target, y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ax_decorate_box(ax):\n",
    "    [j.set_linewidth(0) for j in ax.spines.values()]\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", bottom=False, top=False, \n",
    "                   labelbottom=False, left=False, right=False, labelleft=False)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sample = 2\n",
    "\n",
    "fig, AX = plt.subplots(1, 4, figsize=(13, (13-0.2)/4))\n",
    "plt.subplots_adjust(0, 0, 1, 1, hspace=0, wspace=0.1)\n",
    "for ax in AX:\n",
    "    ax = ax_decorate_box(ax)\n",
    "AX[0].pcolormesh(np.mean(test_input[i_sample, ...,], axis=-1), cmap=plt.cm.gray)\n",
    "AX[1].pcolormesh(y_pred[i_sample, ..., 0], cmap=plt.cm.jet)\n",
    "AX[2].pcolormesh(y_pred[i_sample, ..., 1], cmap=plt.cm.jet)\n",
    "AX[3].pcolormesh(y_pred[i_sample, ..., 2], cmap=plt.cm.jet)\n",
    "\n",
    "AX[0].set_title(\"Original\", fontsize=14);\n",
    "AX[1].set_title(\"Pixels belong to the object\", fontsize=14);\n",
    "AX[2].set_title(\"Surrounding pixels\", fontsize=14);\n",
    "AX[3].set_title(\"Bordering pixels\", fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
